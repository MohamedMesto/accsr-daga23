{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_folder = \"../../data/en/acc_split\"\n",
    "train_folder = \"../../data/en/train_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results = dict()\n",
    "for file in os.listdir(acc_folder):\n",
    "    acc = os.path.splitext(file)[0]\n",
    "    data = json.load(open(os.path.join(acc_folder, file)))\n",
    "    n_samples, dur = 0, 0\n",
    "    for obj in data:\n",
    "        n_samples += 1\n",
    "        dur += obj[\"duration\"]\n",
    "    acc_results[acc] = {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"dur\": round(dur / 3600, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': {'n_samples': 40897, 'dur': 79.01},\n",
       " 'uk': {'n_samples': 134126, 'dur': 178.34},\n",
       " 'hk': {'n_samples': 4260, 'dur': 5.78},\n",
       " 'us': {'n_samples': 382626, 'dur': 520.61},\n",
       " 'sg': {'n_samples': 3365, 'dur': 4.71},\n",
       " 'au': {'n_samples': 51108, 'dur': 71.39},\n",
       " 'ni': {'n_samples': 5968, 'dur': 7.96},\n",
       " 'in': {'n_samples': 99613, 'dur': 148.05},\n",
       " 'ca': {'n_samples': 59342, 'dur': 85.63},\n",
       " 'za': {'n_samples': 8374, 'dur': 11.58},\n",
       " 'nz': {'n_samples': 11877, 'dur': 15.75},\n",
       " 'ph': {'n_samples': 5105, 'dur': 7.36},\n",
       " 'ie': {'n_samples': 9461, 'dur': 12.98},\n",
       " 'sc': {'n_samples': 15474, 'dur': 24.46}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = dict()\n",
    "for file in os.listdir(train_folder):\n",
    "    prepend, acc = os.path.splitext(file)[0].split(\"_\")\n",
    "    if acc not in train_results:\n",
    "        train_results[acc] = dict()\n",
    "    dur, n_samples = 0, 0\n",
    "    fname = os.path.join(train_folder, file)\n",
    "    try:\n",
    "        for line in open(fname):\n",
    "            n_samples += 1\n",
    "            dur += json.loads(line)[\"duration\"]\n",
    "        train_results\n",
    "        train_results[acc][prepend] = {\n",
    "            \"n_samples\": n_samples,\n",
    "            \"dur\": round(dur / 3600, 2)\n",
    "        }\n",
    "    except Exception:  # file does not exist\n",
    "        print(f\"{fname} does not exist\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ni': {'test': {'n_samples': 5968, 'dur': 7.96}},\n",
       " 'uk': {'train': {'n_samples': 107301, 'dur': 142.74},\n",
       "  'test': {'n_samples': 26825, 'dur': 35.6}},\n",
       " 'us': {'test': {'n_samples': 32402, 'dur': 44.13},\n",
       "  'train': {'n_samples': 129608, 'dur': 176.48},\n",
       "  'pretrain': {'n_samples': 220616, 'dur': 300.0}},\n",
       " 'au': {'test': {'n_samples': 51108, 'dur': 71.39}},\n",
       " 'nz': {'test': {'n_samples': 11877, 'dur': 15.75}},\n",
       " 'ca': {'train': {'n_samples': 47474, 'dur': 68.57},\n",
       "  'test': {'n_samples': 11868, 'dur': 17.06}},\n",
       " 'hk': {'test': {'n_samples': 4260, 'dur': 5.78}},\n",
       " 'in': {'test': {'n_samples': 19923, 'dur': 29.61},\n",
       "  'train': {'n_samples': 79690, 'dur': 118.44}},\n",
       " 'sc': {'test': {'n_samples': 15474, 'dur': 24.46}},\n",
       " 'sg': {'test': {'n_samples': 3365, 'dur': 4.71}},\n",
       " 'de': {'train': {'n_samples': 32718, 'dur': 63.19},\n",
       "  'test': {'n_samples': 8179, 'dur': 15.82}},\n",
       " 'ph': {'test': {'n_samples': 5105, 'dur': 7.36}},\n",
       " 'ie': {'test': {'n_samples': 9461, 'dur': 12.98}},\n",
       " 'za': {'test': {'n_samples': 8374, 'dur': 11.58}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ni 8.0 0.01\n",
      "uk 178.3 0.15\n",
      "us 520.6 0.44\n",
      "au 71.4 0.06\n",
      "nz 15.8 0.01\n",
      "ca 85.6 0.07\n",
      "hk 5.8 0.0\n",
      "in 148.1 0.13\n",
      "sc 24.5 0.02\n",
      "sg 4.7 0.0\n",
      "de 79.0 0.07\n",
      "ph 7.4 0.01\n",
      "ie 13.0 0.01\n",
      "za 11.6 0.01\n"
     ]
    }
   ],
   "source": [
    "# compute the distribution of data across accents\n",
    "hours = [0 for _ in range(len(train_results))]\n",
    "for i, acc in enumerate(train_results):\n",
    "    for file in train_results[acc].values():\n",
    "        hours[i] += file[\"dur\"]\n",
    "\n",
    "for i, acc in enumerate(train_results):\n",
    "    print(acc, round(hours[i], 1), round(hours[i] / sum(hours), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57485ebec7cc786341bb1b2b19abc916613018ee2fed0a584a0df93216af615b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
